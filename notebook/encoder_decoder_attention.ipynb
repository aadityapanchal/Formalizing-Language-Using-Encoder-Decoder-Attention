{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import plotly.figure_factory as ff\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the required files\n",
    "\n",
    "train = pd.read_csv('../data/processed/train.csv', index_col=[0])\n",
    "validation = pd.read_csv('../data/processed/validation.csv', index_col=[0])\n",
    "test = pd.read_csv('../data/processed/test.csv', index_col=[0])\n",
    "\n",
    "with open('../model/tokenizer.pkl', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoder_input</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;Er... nope. Sad to say,I'm quite a loner. So ...</td>\n",
       "      <td>&lt;Nope. Sad to say, I'm quite a loner. So just ...</td>\n",
       "      <td>Nope. Sad to say, I'm quite a loner. So just p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;Hope so... call ya when i'm better :)&gt;</td>\n",
       "      <td>&lt;Hope so. I will call you when I'm better.</td>\n",
       "      <td>Hope so. I will call you when I'm better.&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;I ' ll meet you before tne letter Than.&gt;</td>\n",
       "      <td>&lt;I'll meet you before the lecture then.</td>\n",
       "      <td>I'll meet you before the lecture then.&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;Ok, since everyone give notice make it on Mon...</td>\n",
       "      <td>&lt;Ok, since everyone can make it on Monday. Let...</td>\n",
       "      <td>Ok, since everyone can make it on Monday. Let'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;Hm... Wawa suggested we go watch movie then d...</td>\n",
       "      <td>&lt;Wawa suggested that we go watch movie then di...</td>\n",
       "      <td>Wawa suggested that we go watch movie then dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7095</th>\n",
       "      <td>&lt;N probleme! Close fremds call me hammy. Haha,...</td>\n",
       "      <td>&lt;No problem! Close friends call me hammy. Haha...</td>\n",
       "      <td>No problem! Close friends call me hammy. Haha,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7096</th>\n",
       "      <td>&lt;Do uou have ICQ or MSN? Wath ius your emai? Y...</td>\n",
       "      <td>&lt;Do you have ICQ or MSN? What is your email? Y...</td>\n",
       "      <td>Do you have ICQ or MSN? What is your email? Yi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7097</th>\n",
       "      <td>&lt;yupz...if u cant den i help u collect lor...&gt;</td>\n",
       "      <td>&lt;Yes, if you can't then I'll help you to collect.</td>\n",
       "      <td>Yes, if you can't then I'll help you to collect.&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7098</th>\n",
       "      <td>&lt;I don ' at shink soooo. Just bring moey and y...</td>\n",
       "      <td>&lt;I don't think so. Just bring money and yourself.</td>\n",
       "      <td>I don't think so. Just bring money and yourself.&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7099</th>\n",
       "      <td>&lt;Wath ' so your password?&gt;</td>\n",
       "      <td>&lt;What's your password?</td>\n",
       "      <td>What's your password?&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          encoder_input  \\\n",
       "0     <Er... nope. Sad to say,I'm quite a loner. So ...   \n",
       "1               <Hope so... call ya when i'm better :)>   \n",
       "2             <I ' ll meet you before tne letter Than.>   \n",
       "3     <Ok, since everyone give notice make it on Mon...   \n",
       "4     <Hm... Wawa suggested we go watch movie then d...   \n",
       "...                                                 ...   \n",
       "7095  <N probleme! Close fremds call me hammy. Haha,...   \n",
       "7096  <Do uou have ICQ or MSN? Wath ius your emai? Y...   \n",
       "7097     <yupz...if u cant den i help u collect lor...>   \n",
       "7098  <I don ' at shink soooo. Just bring moey and y...   \n",
       "7099                         <Wath ' so your password?>   \n",
       "\n",
       "                                          decoder_input  \\\n",
       "0     <Nope. Sad to say, I'm quite a loner. So just ...   \n",
       "1            <Hope so. I will call you when I'm better.   \n",
       "2               <I'll meet you before the lecture then.   \n",
       "3     <Ok, since everyone can make it on Monday. Let...   \n",
       "4     <Wawa suggested that we go watch movie then di...   \n",
       "...                                                 ...   \n",
       "7095  <No problem! Close friends call me hammy. Haha...   \n",
       "7096  <Do you have ICQ or MSN? What is your email? Y...   \n",
       "7097  <Yes, if you can't then I'll help you to collect.   \n",
       "7098  <I don't think so. Just bring money and yourself.   \n",
       "7099                             <What's your password?   \n",
       "\n",
       "                                         decoder_output  \n",
       "0     Nope. Sad to say, I'm quite a loner. So just p...  \n",
       "1            Hope so. I will call you when I'm better.>  \n",
       "2               I'll meet you before the lecture then.>  \n",
       "3     Ok, since everyone can make it on Monday. Let'...  \n",
       "4     Wawa suggested that we go watch movie then dis...  \n",
       "...                                                 ...  \n",
       "7095  No problem! Close friends call me hammy. Haha,...  \n",
       "7096  Do you have ICQ or MSN? What is your email? Yi...  \n",
       "7097  Yes, if you can't then I'll help you to collect.>  \n",
       "7098  I don't think so. Just bring money and yourself.>  \n",
       "7099                             What's your password?>  \n",
       "\n",
       "[7100 rows x 3 columns]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __padding__(self, sequence):\n",
    "        return pad_sequences(sequence, maxlen = self.max_length, dtype = 'int32', padding = 'post')\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        self.encoder_input_sequence = self.tokenizer['informal'].texts_to_sequences([self.data['encoder_input'].values[i]])\n",
    "        self.decoder_input_sequence = self.tokenizer['normalized'].texts_to_sequences([self.data['decoder_input'].values[i]])\n",
    "        self.decoder_output_sequence = self.tokenizer['normalized'].texts_to_sequences([self.data['decoder_output'].values[i]])\n",
    "        return self.__padding__(self.encoder_input_sequence), self.__padding__(self.decoder_input_sequence), self.__padding__(self.decoder_output_sequence)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoder_inps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(tf.keras.utils.Sequence):    \n",
    "    def __init__(self, dataset, batch_size = 1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.dataset.data['encoder_input'].values))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = [self.dataset[idx] for idx in range(i * self.batch_size, (i + 1) * self.batch_size)]\n",
    "        batch = [np.squeeze(np.stack(samples, axis = 1), axis = 0) for samples in zip(*data)]\n",
    "        return tuple([[batch[0],batch[1]],batch[2]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, inp_vocab_size, embedding_dim, lstm_size, input_length):\n",
    "        super().__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        embedding_params = {'input_dim': inp_vocab_size,\n",
    "                            'output_dim': embedding_dim,\n",
    "                            'embeddings_initializer' : tf.keras.initializers.RandomNormal(mean = 0, stddev = 1, seed = 42),\n",
    "                            'input_length' : input_length, \n",
    "                            'mask_zero' : True}\n",
    "        \n",
    "        lstm_params = {'units':self.lstm_size, \n",
    "                      'return_state' : True, \n",
    "                      'return_sequences' : True,\n",
    "                      'kernel_initializer' : tf.keras.initializers.glorot_uniform(seed = 42),\n",
    "                      'recurrent_initializer' : tf.keras.initializers.orthogonal(seed = 42)}\n",
    "        \n",
    "  \n",
    "        self.embedding = Embedding(**embedding_params)\n",
    "        self.lstm1 = LSTM(**lstm_params)\n",
    "        self.lstm2 = LSTM(**lstm_params)\n",
    "\n",
    "    def call(self, input):\n",
    "        self.encoder_output, self.hidden_state, self.current_state = self.lstm1(self.embedding(input[0]), initial_state = input[1])\n",
    "        return self.lstm2(self.encoder_output, [self.hidden_state, self.current_state])\n",
    "    \n",
    "    def initialize_states(self, batch_size):\n",
    "      return tf.zeros([batch_size, self.lstm_size]), tf.zeros([batch_size, self.lstm_size])\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, lstm_size, scoring_function):\n",
    "        super(Attention, self).__init__()\n",
    "        self.scoring_function = scoring_function\n",
    "       \n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        self.W = tf.keras.layers.Dense(lstm_size)   \n",
    "        self.W1 = tf.keras.layers.Dense(lstm_size)\n",
    "        self.W2 = tf.keras.layers.Dense(lstm_size)\n",
    "        self.V1 = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self,input):        \n",
    "        score = self.V(tf.linalg.matmul(input[1], tf.expand_dims(input[0], 1), transpose_b=True)) if self.scoring_function == 'dot' else (\n",
    "            tf.keras.layers.Dot(axes=(2, 1))([self.W(input[1]), tf.expand_dims(input[0], axis = 2)]) if self.scoring_function == 'general' else \n",
    "            self.V1(tf.nn.tanh(self.W1(tf.expand_dims(input[0], 1)) + self.W2(input[1])))\n",
    "        )\n",
    "        return tf.reduce_sum(tf.nn.softmax(score, axis=1) * input[1], axis=1), tf.nn.softmax(score, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step_Decoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, out_vocab_size, embedding_dim, input_length, lstm_size, scoring_function, embedding_matrix = None):\n",
    "        # Initialize the parameters\n",
    "        super().__init__()\n",
    "        self.attention = Attention(lstm_size, scoring_function)\n",
    "        \n",
    "        embedding_params = {'input_dim' : out_vocab_size, 'output_dim' : embedding_dim,\n",
    "                                       'embeddings_initializer' : tf.keras.initializers.RandomNormal(mean = 0, stddev = 1, seed = 42),\n",
    "                                       'input_length' : input_length, 'mask_zero' : True}\n",
    "        lstm_params = {'units':lstm_size, 'return_state' : True, 'return_sequences' : True, \n",
    "                            'kernel_initializer' : tf.keras.initializers.glorot_uniform(seed = 42), \n",
    "                            'recurrent_initializer' : tf.keras.initializers.orthogonal(seed = 42)}\n",
    "        \n",
    "        if embedding_matrix:\n",
    "            embedding_params['embeddings_initializer'] = tf.keras.initializers.Constant(embedding_matrix)\n",
    "            embedding_params['trainable'] = False\n",
    "        \n",
    "        self.embedding = Embedding(**embedding_params)\n",
    "        self.lstm1 = LSTM(**lstm_params)\n",
    "        self.lstm2 = LSTM(**lstm_params)\n",
    "        self.dense = Dense(out_vocab_size)\n",
    "\n",
    "\n",
    "    def call(self, input):\n",
    "\n",
    "        encoder_hidden = input[2]\n",
    "        encoder_current = input[3]\n",
    "        dec_output, encoder_hidden, encoder_current = self.lstm1(tf.concat([tf.expand_dims(self.attention([encoder_hidden, input[1]])[0], 1), \n",
    "                                                                            self.embedding(input[0])], axis = -1), [encoder_hidden, encoder_current])\n",
    "        dec_output, encoder_hidden, encoder_current = self.lstm2(dec_output, [encoder_hidden, encoder_current])\n",
    "        output = self.dense(tf.reshape(dec_output, (-1, dec_output.shape[2])))\n",
    "        \n",
    "        return output, encoder_hidden, encoder_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, out_vocab_size, embedding_dim, input_length, lstm_size, scoring_function, embedding_matrix = None):\n",
    "        super().__init__()\n",
    "        self.timestepdecoder = Step_Decoder(out_vocab_size, embedding_dim, input_length,\n",
    "                                                lstm_size, scoring_function, embedding_matrix)\n",
    "        \n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, input):\n",
    "        outputs = tf.TensorArray(tf.float32, size = tf.shape(input[0])[1])\n",
    "        for timestep in range(tf.shape(input[0])[1]):\n",
    "            outputs = outputs.write(timestep, self.timestepdecoder([input[0][:, timestep:timestep+1], input[1], input[2], input[3]])[0])\n",
    "        \n",
    "        return tf.transpose(outputs.stack(), [1,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, input_length, inp_vocab_size, out_vocab_size, lstm_size, scoring_function, batch_size, embedding_dim, embedding_matrix = None):\n",
    "    \n",
    "        super().__init__()\n",
    "    \n",
    "        encoder_args = {'inp_vocab_size' : inp_vocab_size + 1, 'embedding_dim' : embedding_dim, 'lstm_size' : lstm_size, 'input_length' : input_length}\n",
    "        decoder_args = {'out_vocab_size' : out_vocab_size + 1, 'embedding_dim' : embedding_dim, 'lstm_size' : lstm_size,\n",
    "                               'scoring_function' : scoring_function, 'input_length' : input_length, 'embedding_matrix' : embedding_matrix}\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder = Encoder(**encoder_args)\n",
    "        self.decoder = Decoder(**decoder_args)\n",
    "    \n",
    "    def call(self, data):\n",
    "        encoder_output, encoder_hidden, encoder_current = self.encoder([data[0], self.encoder.initialize_states(self.batch_size)])\n",
    "        return self.decoder([data[1], encoder_output, encoder_hidden, encoder_current])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n",
    "\n",
    "@tf.function\n",
    "def loss_function(real, pred):\n",
    "    # Refer https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensorboard_cb(model):\n",
    "    root_logdir = os.path.join(os.curdir, model)\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    logdir = os.path.join(root_logdir, run_id)\n",
    "    return tf.keras.callbacks.TensorBoard(logdir, histogram_freq = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UNITS = 200\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 200 \n",
    "TRAIN_STEPS = train.shape[0]//BATCH_SIZE\n",
    "VALID_STEPS = validation.shape[0]//BATCH_SIZE\n",
    "train_dataset = Dataset(train, tokenizer, MAX_LEN)\n",
    "validation_dataset  = Dataset(validation, tokenizer, MAX_LEN)\n",
    "train_dataloader = Dataloader(train_dataset, BATCH_SIZE)\n",
    "validation_dataloader = Dataloader(validation_dataset, BATCH_SIZE)\n",
    "\n",
    "cb_params = {'monitor' : 'val_loss', \n",
    "                  'factor' : 0.5, \n",
    "                  'verbose' : 1, \n",
    "                  'patience' : 1, \n",
    "                  'min_lr' : 0.0001}\n",
    "cb_stopper_cb = {'monitor' : 'val_loss', \n",
    "                 'patience' : 3, \n",
    "                 'verbose' : 1, \n",
    "                 'restore_best_weights' : True}\n",
    "\n",
    "model_dot  = Encoder_Decoder(input_length = MAX_LEN, inp_vocab_size = len(tokenizer['informal'].word_index.keys()),\n",
    "                                            out_vocab_size = len(tokenizer['normalized'].word_index.keys()), lstm_size = UNITS,\n",
    "                                            scoring_function = 'dot', batch_size = BATCH_SIZE,\n",
    "                                            embedding_dim = len(tokenizer['normalized'].word_index.keys()), embedding_matrix = None)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "model_dot.compile(optimizer = optimizer, loss = loss_function)\n",
    "\n",
    "learning_rate_cb = tf.keras.callbacks.ReduceLROnPlateau(**cb_params)\n",
    "tensorboard_cb = create_tensorboard_cb(\"Model_Dot_logs\")\n",
    "stopper_cb = tf.keras.callbacks.EarlyStopping(**cb_stopper_cb)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"Model_Dot\",\n",
    "                                                    save_best_only = True, save_weights_only = False)\n",
    "\n",
    "model_dot.fit(train_dataloader, steps_per_epoch = TRAIN_STEPS, epochs = EPOCHS,\n",
    "            callbacks = [learning_rate_cb, tensorboard_cb, stopper_cb, checkpoint_cb],\n",
    "            validation_data = validation_dataloader, validation_steps = VALID_STEPS)\n",
    "model_dot.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(input_sentence, model):\n",
    "    inputs = tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences([[tokenizer['informal'].word_index.get(i, 0) \n",
    "                                                                                  for i in input_sentence]], maxlen = MAX_LEN, padding = 'post'))\n",
    "    sentence = ''\n",
    "    enc_out, state_h, state_c = model.encoder([inputs, (tf.zeros([1, UNITS]), tf.zeros([1, UNITS]))])\n",
    "    dec_input = tf.expand_dims([tokenizer['normalized'].word_index['<']], 0)\n",
    "    for _ in range(MAX_LEN):\n",
    "        output, state_h, state_c = model.decoder.timestepdecoder([dec_input, enc_out, state_h, state_c])\n",
    "        character = tokenizer['normalized'].index_word.get(tf.argmax(output[0]).numpy(), '')\n",
    "        if character == '>':\n",
    "            break\n",
    "        else:\n",
    "            sentence += character\n",
    "            dec_input = tf.expand_dims([tf.argmax(output[0]).numpy()], 0)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score for the predictions: 3.3823417436993433e-233\n"
     ]
    }
   ],
   "source": [
    "def post_processing(s):\n",
    "    if s.startswith('<'):\n",
    "        s = s[len('<'):]\n",
    "    if s.endswith('>'):\n",
    "        s = s[:-len('>')]\n",
    "    return s\n",
    "\n",
    "def predictor(s):\n",
    "    return predict(s, model_dot)\n",
    "\n",
    "def convert_formals(s):\n",
    "    return [s.split()]\n",
    "\n",
    "def convert_predictions(s):\n",
    "    return s.split()\n",
    "\n",
    "test['informals'] = test['encoder_input'].apply(post_processing)\n",
    "test['formals'] = test['decoder_input'].apply(post_processing)\n",
    "test['predictions'] = test['informals'].apply(predictor)\n",
    "test['formals'] = test['formals'].apply(convert_formals)\n",
    "test['predictions'] = test['predictions'].apply(convert_predictions)\n",
    "\n",
    "bleu_scores = []\n",
    "i = 0\n",
    "\n",
    "while i < (len(test)):\n",
    "    bleu_scores.append(sentence_bleu(test['formals'].iloc[i], test['predictions'].iloc[i]))\n",
    "    i = i + 1\n",
    "\n",
    "print('Average BLEU score for the predictions:', np.mean(bleu_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ff.create_distplot([bleu_scores], ['Count'])\n",
    "fig.update_layout(title= 'BLEU Score')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_general  = Encoder_Decoder(input_length = MAX_LEN, inp_vocab_size = len(tokenizer['informal'].word_index.keys()),\n",
    "                                            out_vocab_size = len(tokenizer['normalized'].word_index.keys()), lstm_size = UNITS,\n",
    "                                            scoring_function = 'general', batch_size = BATCH_SIZE,\n",
    "                                            embedding_dim = len(tokenizer['normalized'].word_index.keys()), embedding_matrix = None)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "model_general.compile(optimizer = optimizer, loss = loss_function)\n",
    "\n",
    "learning_rate_cb = tf.keras.callbacks.ReduceLROnPlateau(**cb_params)\n",
    "tensorboard_cb = create_tensorboard_cb(\"Model_General_logs\")\n",
    "stopper_cb = tf.keras.callbacks.EarlyStopping(**cb_stopper_cb)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"Model_General\",\n",
    "                                                    save_best_only = True, save_weights_only = False)\n",
    "\n",
    "model_general.fit(train_dataloader, steps_per_epoch = TRAIN_STEPS, epochs = EPOCHS,\n",
    "            callbacks = [learning_rate_cb, tensorboard_cb, stopper_cb, checkpoint_cb],\n",
    "            validation_data = validation_dataloader, validation_steps = VALID_STEPS)\n",
    "model_general.summary()\n",
    "\n",
    "def predictor(s):\n",
    "    return predict(s, model_general)\n",
    "\n",
    "test['informals'] = test['encoder_input'].apply(post_processing)\n",
    "test['formals'] = test['decoder_input'].apply(post_processing)\n",
    "test['predictions'] = test['informals'].apply(predictor)\n",
    "test['formals'] = test['formals'].apply(convert_formals)\n",
    "test['predictions'] = test['predictions'].apply(convert_predictions)\n",
    "\n",
    "bleu_scores = []\n",
    "i = 0\n",
    "\n",
    "while i < (len(test)):\n",
    "    bleu_scores.append(sentence_bleu(test['formals'].iloc[i], test['predictions'].iloc[i]))\n",
    "    i = i + 1\n",
    "\n",
    "print('Average BLEU score for the predictions:', np.mean(bleu_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ff.create_distplot([bleu_scores], ['Count'])\n",
    "fig.update_layout(title= 'BLEU Score')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_concat  = Encoder_Decoder(input_length = MAX_LEN, inp_vocab_size = len(tokenizer['informal'].word_index.keys()),\n",
    "                                            out_vocab_size = len(tokenizer['normalized'].word_index.keys()), lstm_size = UNITS,\n",
    "                                            scoring_function = 'concat', batch_size = BATCH_SIZE,\n",
    "                                            embedding_dim = len(tokenizer['normalized'].word_index.keys()), embedding_matrix = None)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "model_concat.compile(optimizer = optimizer, loss = loss_function)\n",
    "\n",
    "learning_rate_cb = tf.keras.callbacks.ReduceLROnPlateau(**cb_params)\n",
    "tensorboard_cb = create_tensorboard_cb(\"Model_Concat_logs\")\n",
    "stopper_cb = tf.keras.callbacks.EarlyStopping(**cb_stopper_cb)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"Model_Concat\",\n",
    "                                                    save_best_only = True, save_weights_only = False)\n",
    "\n",
    "model_concat.fit(train_dataloader, steps_per_epoch = TRAIN_STEPS, epochs = EPOCHS,\n",
    "            callbacks = [learning_rate_cb, tensorboard_cb, stopper_cb, checkpoint_cb],\n",
    "            validation_data = validation_dataloader, validation_steps = VALID_STEPS)\n",
    "model_concat.summary()\n",
    "\n",
    "def predictor(s):\n",
    "    return predict(s, model_concat)\n",
    "\n",
    "test['informals'] = test['encoder_input'].apply(post_processing)\n",
    "test['formals'] = test['decoder_input'].apply(post_processing)\n",
    "test['predictions'] = test['informals'].apply(predictor)\n",
    "test['formals'] = test['formals'].apply(convert_formals)\n",
    "test['predictions'] = test['predictions'].apply(convert_predictions)\n",
    "\n",
    "bleu_scores = []\n",
    "i = 0\n",
    "\n",
    "while i < (len(test)):\n",
    "    bleu_scores.append(sentence_bleu(test['formals'].iloc[i], test['predictions'].iloc[i]))\n",
    "    i = i + 1\n",
    "\n",
    "print('Average BLEU score for the predictions:', np.mean(bleu_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ff.create_distplot([bleu_scores], ['Count'])\n",
    "fig.update_layout(title= 'BLEU Score')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Predictions:\n",
      "Informal Input: How you doing?\n",
      "Expected Output: How you doing?\n",
      "Predicted Output:  How you doing?\n",
      "Bleu Score of Prediction : 1.0\n",
      "\n",
      "\n",
      "Worst Predictions:\n",
      "Informal Input : Kid's shop selling clothes izit...\n",
      "Expected Output : Kid's shop is selling clothes, is it?\n",
      "Predicted Output : I'm still to some to see you all not.\n",
      "Bleu Score of Prediction : 0.00\n"
     ]
    }
   ],
   "source": [
    "scores = np.array(bleu_scores)\n",
    "indices = (np.argsort(scores)).tolist()\n",
    "worst = indices[0]\n",
    "best = indices[-1]\n",
    "\n",
    "print('Best Predictions:')\n",
    "print('Informal Input: ',test['informals'].iloc[indices[-1]])\n",
    "print('Expected Output: ',test['formals'].iloc[indices[-1]][0])\n",
    "print('Predicted Output: ',test['predictions'].iloc[indices[-1]])\n",
    "print('Bleu Score of Prediction: ',scores[indices[-1]])\n",
    "print(\"\\n\")\n",
    "\n",
    "print('Worst Predictions:')\n",
    "print('Informal Input: ',test['informals'].iloc[indices[0]])\n",
    "print('Expected Output: ',test['formals'].iloc[indices[0]][0])\n",
    "print('Predicted Output: ',test['predictions'].iloc[indices[0]])\n",
    "print('Bleu Score of Prediction: ',scores[indices[0]])\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
